{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vårt projekt\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x273fff57e10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "DEVICE = torch.device(device)\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alice paths\n",
    "#data_path = '/Users/alicetottie/Downloads/projekt_data/train_paracrawl.en'\n",
    "#data_path_2 = '/Users/alicetottie/Downloads/projekt_data/train_paracrawl.sv'\n",
    "\n",
    "#Maja paths\n",
    "# data_path = '/Users/majarygard/Documents/LTH/Projekt i Data/train_paracrawl.en'\n",
    "# data_path_2 = '/Users/majarygard/Documents/LTH/Projekt i Data/train_paracrawl.sv'\n",
    "\n",
    "#Pernilla paths\n",
    "data_path = 'C:\\\\Users\\\\nilla\\\\Plugg\\\\project-data\\\\train_paracrawl.en\\\\train_paracrawl.en'\n",
    "data_path_2 = 'C:\\\\Users\\\\nilla\\\\Plugg\\\\project-data\\\\train_paracrawl.sv\\\\train_paracrawl.sv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "with open(data_path_2, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines2 = f.read().split(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4960283"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4960283"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text= line.split(\"\\t\")\n",
    "    input_texts.append(input_text)\n",
    "\n",
    "with open(\"input_texts.txt\", \"w\") as f:\n",
    "    for input_text in input_texts:\n",
    "        f.write('\\t'.join(input_text) + '\\n')\n",
    "    \n",
    "for line in lines2[: min(num_samples, len(lines2) - 1)]:\n",
    "    target_text= line.split(\"\\t\")\n",
    "    target_texts.append(target_text)  \n",
    "\n",
    "with open(\"target_texts.txt\", \"w\") as f:\n",
    "    for target_text in target_texts:\n",
    "        f.write('\\t'.join(target_text) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\" This is my third course and I bought this concept a long time ago.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\" Det här är min tredje kurs och jag har köpt det hela för länge sedan.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PERCENTAGE = 0.8\n",
    "train_val = int(TRAIN_PERCENTAGE * num_samples)\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = list(zip(input_texts, target_texts))\n",
    "random.shuffle(text_pairs)\n",
    "input_texts, target_texts = zip(*text_pairs)\n",
    "input_texts, target_texts = list(input_texts), list(target_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_texts = input_texts[:train_val]\n",
    "train_target_texts = target_texts[:train_val]\n",
    "\n",
    "val_input_texts = input_texts[train_val:]\n",
    "val_target_texts = target_texts[train_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁This', '▁is', '▁a', '▁t', 'est', '</s>']\n",
      "[1, 665, 64, 5, 4, 134, 2]\n",
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=input_texts.txt --model_prefix=m --vocab_size=2000 --model_type=bpe --pad_id=3')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "sp.SetEncodeExtraOptions(\"bos:eos\")\n",
    "sp.SetDecodeExtraOptions(\"bos:eos\")\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
    "print(sp.decode_ids([1, 665, 64, 5, 4, 134, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2codes(texts, sp):\n",
    "    codes = []\n",
    "    for text in texts:\n",
    "        text_l = text[0]\n",
    "        code = sp.encode_as_ids(text_l)\n",
    "        n_code = torch.tensor(code)\n",
    "        codes.append(n_code)\n",
    "\n",
    "    return codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[\"!!TheM!Z!! hasn't added any friends yet!\"],\n",
       "  ['\" Memory / Summerday\" gouache/wood/butterfly 48x75 / 48x92cm 2006'],\n",
       "  ['\" child safety in Egypt \"\" harmfulness of the sun in Egypt on baby skin \"\" what medications the child in Egypt \"\" where on holiday with his family to Egypt \"\" pediatrician \"\" in which the child free hotels \"']],\n",
       " [['!!TheM!Z!! har inte lagt till några vänner ännu!'],\n",
       "  ['\"Hågkomst / Sommardag\" gouache/trä 48x75 /48x92cm2006'],\n",
       "  ['\" barnsäkerhet i Egypten \"\" skadlighet solen i Egypten på baby hud \"\" vilka mediciner barnet i Egypten \"\" där på semester med sin familj till Egypten \"\" barnläkare \"\" där barnet hotellsökningen \"']])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_texts[:3], train_target_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   1,  314,  888, 1928, 1938, 1976,  460,  342, 1945, 1900, 1785,  242,\n",
       "         1041,  304, 1938,    2]),\n",
       " tensor([   1,    8,   67,  171,  354,  464,   50,  130,  498,  894, 1911,   54,\n",
       "           27,   71,    6, 1966, 1428, 1966,  891,  103, 1914,  108,  393, 1962,\n",
       "         1951, 1957, 1943,  464,  393, 1962, 1951, 1582,  964,  341, 1956,    2]),\n",
       " tensor([   1,    8,  224,  396, 1260,  608,   34,  154,   78,   37,  414,  296,\n",
       "          437,   23,    9, 1263,   34,  154,   89,   25, 1612, 1770,   78,  280,\n",
       "         1792,  406,    9,  224,  396,   34,  154,   78,  709,   89, 1554,  111,\n",
       "          246, 1824,   31,  154,   78,   36,   40, 1903,   17, 1636,  431,   78,\n",
       "           34,  226,    9,  224,  396,   21,  434, 1036,    8,    2])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2codes(train_input_texts[:3], sp)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes2text(codes):\n",
    "    texts = []\n",
    "    for code in codes:\n",
    "        code_l = list(code)\n",
    "        for p in code_l:\n",
    "            texts.append(sp.id_to_piece(p.item()))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁!!',\n",
       " 'The',\n",
       " 'M',\n",
       " '!',\n",
       " 'Z',\n",
       " '!!',\n",
       " '▁hasn',\n",
       " \"'\",\n",
       " 't',\n",
       " '▁added',\n",
       " '▁any',\n",
       " '▁friends',\n",
       " '▁yet',\n",
       " '!',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '▁\"',\n",
       " '▁M',\n",
       " 'em',\n",
       " 'ory',\n",
       " '▁/',\n",
       " '▁S',\n",
       " 'um',\n",
       " 'mer',\n",
       " 'day',\n",
       " '\"',\n",
       " '▁g',\n",
       " 'ou',\n",
       " 'ac',\n",
       " 'he',\n",
       " '/',\n",
       " 'wood',\n",
       " '/',\n",
       " 'but',\n",
       " 'ter',\n",
       " 'f',\n",
       " 'ly',\n",
       " '▁4',\n",
       " '8',\n",
       " 'x',\n",
       " '7',\n",
       " '5',\n",
       " '▁/',\n",
       " '▁4',\n",
       " '8',\n",
       " 'x',\n",
       " '92',\n",
       " 'cm',\n",
       " '▁200',\n",
       " '6',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '▁\"',\n",
       " '▁ch',\n",
       " 'ild',\n",
       " '▁saf',\n",
       " 'ety',\n",
       " '▁in',\n",
       " '▁Egypt',\n",
       " '▁\"\"',\n",
       " '▁h',\n",
       " 'arm',\n",
       " 'ful',\n",
       " 'ness',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁sun',\n",
       " '▁in',\n",
       " '▁Egypt',\n",
       " '▁on',\n",
       " '▁b',\n",
       " 'aby',\n",
       " '▁skin',\n",
       " '▁\"\"',\n",
       " '▁what',\n",
       " '▁medic',\n",
       " 'ations',\n",
       " '▁the',\n",
       " '▁ch',\n",
       " 'ild',\n",
       " '▁in',\n",
       " '▁Egypt',\n",
       " '▁\"\"',\n",
       " '▁where',\n",
       " '▁on',\n",
       " '▁holiday',\n",
       " '▁with',\n",
       " '▁his',\n",
       " '▁family',\n",
       " '▁to',\n",
       " '▁Egypt',\n",
       " '▁\"\"',\n",
       " '▁p',\n",
       " 'ed',\n",
       " 'i',\n",
       " 'at',\n",
       " 'ric',\n",
       " 'ian',\n",
       " '▁\"\"',\n",
       " '▁in',\n",
       " '▁which',\n",
       " '▁the',\n",
       " '▁ch',\n",
       " 'ild',\n",
       " '▁f',\n",
       " 'ree',\n",
       " '▁hotels',\n",
       " '▁\"',\n",
       " '</s>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes2text(text2codes(train_input_texts[:3], sp)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super().__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)\n",
    "                        * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding * math.sqrt(self.emb_size)\n",
    "                            + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 0.0000, 0.0000, 1.1111,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 0.0000,\n",
       "          0.0000, 1.1111],\n",
       "         [0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 1.1111,\n",
       "          0.0000, 1.1111],\n",
       "         [0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 0.0000,\n",
       "          0.0000, 1.1111],\n",
       "         [0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 1.1111, 0.0000, 1.1111,\n",
       "          0.0000, 1.1111]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe(torch.zeros(1, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        # Same source and target embs Sect. 3.4\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, emb_size, padding_idx=3)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        # Bias to be compatible with embeddings\n",
    "        self.generator = nn.Linear(emb_size, vocab_size, bias=False)\n",
    "        self.generator.weight = self.embedding.weight  # Shared weights Sect. 3.4\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.embedding(src))\n",
    "        tgt_emb = self.positional_encoding(self.embedding(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "            self.embedding(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "            self.embedding(tgt)), memory,\n",
    "            tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float(\n",
    "        '-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_square_subsequent_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),\n",
    "                           device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == 3).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == 3).transpose(0, 1).type(torch.float32)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = pad_sequence(text2codes(\n",
    "    train_input_texts[:3], sp), padding_value=3)\n",
    "tgt = pad_sequence(text2codes(\n",
    "    train_target_texts[:3], sp), padding_value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[\"!!TheM!Z!! hasn't added any friends yet!\"],\n",
       "  ['\" Memory / Summerday\" gouache/wood/butterfly 48x75 / 48x92cm 2006'],\n",
       "  ['\" child safety in Egypt \"\" harmfulness of the sun in Egypt on baby skin \"\" what medications the child in Egypt \"\" where on holiday with his family to Egypt \"\" pediatrician \"\" in which the child free hotels \"']],\n",
       " [['!!TheM!Z!! har inte lagt till några vänner ännu!'],\n",
       "  ['\"Hågkomst / Sommardag\" gouache/trä 48x75 /48x92cm2006'],\n",
       "  ['\" barnsäkerhet i Egypten \"\" skadlighet solen i Egypten på baby hud \"\" vilka mediciner barnet i Egypten \"\" där på semester med sin familj till Egypten \"\" barnläkare \"\" där barnet hotellsökningen \"']])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_texts[:3], train_target_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mask(src, tgt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
